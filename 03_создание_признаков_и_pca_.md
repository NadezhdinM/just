---
 title: 'Chapter 3: Создание признаков и PCA'
 layout: home
---

# Chapter 3: Создание признаков и PCA


В [предыдущей главе](02_расчет_рейтинга_elo_.html) мы научились рассчитывать рейтинг ELO для каждого игрока, который динамически отражает его силу с учетом результатов прошлых матчей и силы соперников. Теперь у нас есть оценка общей силы игрока (`elo_1`, `elo_2`) и его силы на конкретном покрытии (`elo_1_surface`, `elo_2_surface`).

Но для точного прогноза нам нужно больше деталей. Одного рейтинга ELO недостаточно, чтобы понять *стиль* игры и текущую форму игрока в конкретных аспектах, таких как подача или прием. В нашем основном DataFrame (`final_df.csv`) есть много "сырой" статистики по каждому матчу: количество эйсов, двойных ошибок, процент попадания первой подачи и так далее. Эти цифры полезны, но их слишком много, и некоторые из них сильно связаны друг с другом.

**Проблема:** Как превратить эту россыпь статистических данных в несколько понятных и информативных показателей, которые наша модель сможет эффективно использовать для прогнозирования?

**Цель этой главы:** Научиться создавать новые, более осмысленные **признаки** (features) из сырой статистики и использовать **Метод Главных Компонент (PCA)**, чтобы сжать группы связанных признаков в компактные числовые показатели, сохраняя при этом максимум информации.

Представьте, что вы анализируете автомобиль. Вместо того чтобы изучать десятки параметров (объем двигателя, ход поршня, передаточные числа коробки передач и т.д.), вы хотите получить несколько ключевых характеристик, например, "мощность/скорость" и "экономичность". Создание признаков и PCA помогают нам сделать нечто похожее для теннисистов.

## 1. Создание признаков (Feature Engineering)

**Что это?** Создание признаков — это процесс генерации новых столбцов (признаков) в нашем наборе данных на основе уже существующих. Вместо того чтобы использовать "сырые" значения (например, количество выигранных очков на первой подаче), мы создаем более информативные показатели (например, *процент* выигранных очков на первой подаче).

**Зачем это нужно?**
*   **Нормализация:** Проценты и отношения часто более сопоставимы между разными матчами, чем абсолютные числа (например, 10 эйсов в коротком матче - это отлично, а в длинном - может быть средне).
*   **Информативность:** Некоторые производные показатели лучше отражают суть происходящего. Например, процент двойных ошибок на второй подаче (`second_serve_errors_1`) может быть важнее, чем просто общее количество двойных ошибок.
*   **Подготовка к PCA:** Для метода главных компонент часто лучше подходят стандартизированные или относительные показатели.

**Примеры создания признаков в проекте:**

В скрипте `Feature_Extraction_Mistakes_on_Service.py` мы рассчитываем показатели ошибок на подаче:

*   **Процент ошибок на первой подаче:** Доля не попавших первых подач.
*   **Процент ошибок на второй подаче (двойные ошибки):** Доля двойных ошибок относительно всех вторых подач.

```python
# Пример расчета процента ошибок на первой подаче для игрока 1
# (из Feature_Extraction_Mistakes_on_Service.py)
# matches_df['first_serve_attempted_1'] - количество попыток первой подачи
# matches_df['first_serve_made_1'] - количество успешных первых подач

# Добавляем новый столбец
matches_df['first_serve_errors_1'] = None # Создаем пустой столбец

# Заполняем его в цикле для каждого матча (i)
# (Упрощенный пример для одной строки)
i = 0 # Пример для первого матча
attempts = matches_df.iloc[i]['first_serve_attempted_1']
made = matches_df.iloc[i]['first_serve_made_1']

if attempts > 0: # Избегаем деления на ноль
    error_rate = 1.0 - (made / attempts)
    matches_df.at[i, 'first_serve_errors_1'] = error_rate
else:
    matches_df.at[i, 'first_serve_errors_1'] = 0.0 # Или None

print(f"Пример расчета процента ошибок на 1-й подаче: {matches_df.at[i, 'first_serve_errors_1']:.2f}")
```

**Объяснение:**
*   Мы создаем новый столбец `first_serve_errors_1`.
*   Для каждой строки (матча) мы берем количество попыток (`attempts`) и успешных подач (`made`).
*   Вычисляем долю ошибок как `1 - (успешные / попытки)`.
*   Записываем результат в новый столбец.

Аналогично в скрипте `Features_Extraction_PCA_Serve_Return_Ratings.py` перед применением PCA создаются другие важные соотношения, характеризующие подачу и прием:

*   Процент попадания первой подачи (`first_serve_made / first_serve_attempted`)
*   Процент выигранных очков на своей подаче (`service_points_won / service_points_attempted`)
*   Процент выигранных очков на первой подаче (`first_serve_points_made / first_serve_points_attempted`)
*   Процент выигранных очков на второй подаче (`second_serve_points_made / second_serve_points_attempted`)
*   Процент эйсов (`aces / service_points_attempted`)
*   Процент выигранных геймов на своей подаче (`service_games_won / total_service_games`)

```python
# Пример расчета признаков для PCA подачи игрока 1
# (из Features_Extraction_PCA_Serve_Return_Ratings.py)

# service_pca - временный DataFrame с нужными столбцами
# Создаем новые столбцы с отношениями
service_pca['0'] = service_pca['first_serve_made_1'] / service_pca['first_serve_attempted_1']
service_pca['1'] = service_pca['service_points_won_1'] / service_pca['service_points_attempted_1']
service_pca['2'] = service_pca['first_serve_points_made_1'] / service_pca['first_serve_points_attempted_1']
# ... и так далее для других показателей ...

# Удаляем исходные столбцы, оставляя только новые признаки
# service_pca.drop(исходные_колонки, axis=1, inplace=True)

print("Созданы новые признаки (соотношения) для анализа подачи:")
print(service_pca[['0', '1', '2']].head()) # Показываем первые строки новых признаков
```

**Объяснение:**
*   Мы берем сырые статистические данные (количество очков, подач и т.д.).
*   Вычисляем на их основе новые признаки в виде долей или процентов.
*   Эти новые признаки (`'0'`, `'1'`, `'2'`, ...) более стандартизированы и готовы для следующего шага – PCA.

Создание таких признаков – важный творческий процесс, где мы пытаемся "подсказать" модели, на какие аспекты игры стоит обратить внимание.

## 2. Метод главных компонент (PCA - Principal Component Analysis)

После создания новых признаков у нас все еще может быть много столбцов, описывающих схожие аспекты игры. Например, несколько разных процентов и соотношений могут характеризовать качество подачи.

**Проблема:**
*   **Избыточность:** Многие признаки могут быть сильно скоррелированы (например, высокий процент выигранных очков на первой подаче часто связан с высоким процентом попадания первой подачи). Это избыточная информация.
*   **Проклятие размерности:** Слишком большое количество признаков может усложнить и замедлить обучение модели, а иногда даже ухудшить ее точность.

**Что такое PCA?** Метод главных компонент (PCA) — это популярный метод **снижения размерности**. Он позволяет "сжать" набор из нескольких связанных (скоррелированных) признаков в меньшее количество (часто один или два) новых, искусственных признаков, называемых **главными компонентами**. При этом PCA старается сохранить как можно больше исходной информации (вариативности) данных.

**Аналогия:** Вернемся к автомобилю. У нас есть данные: максимальная скорость, время разгона 0-100 км/ч, мощность двигателя, крутящий момент. Все они связаны с "быстротой" машины. PCA мог бы объединить их в один компонент – "Индекс скорости". Точно так же PCA может взять несколько показателей подачи (процент попадания, процент выигранных очков на 1-й и 2-й подаче, процент эйсов) и объединить их в один "Рейтинг подачи PCA".

**Как это работает (очень упрощенно)?**
PCA находит новые "направления" (оси) в многомерном пространстве признаков.
1.  Первая главная компонента – это направление, вдоль которого данные имеют наибольший разброс (вариативность).
2.  Вторая главная компонента – это следующее направление, перпендикулярное первому, с максимальным оставшимся разбросом.
3.  И так далее...

Затем PCA проецирует исходные данные на эти новые оси (компоненты). Часто оказывается, что первые несколько компонент "вбирают" в себя большую часть информации из исходных данных.

**Шаг 0: Масштабирование данных (Standardization)**

Перед применением PCA очень важно **масштабировать** данные. Обычно используется `StandardScaler` из библиотеки `sklearn`.

**Зачем?** PCA чувствителен к масштабу признаков. Если один признак имеет значения в тысячах (например, количество очков), а другой – в долях единицы (например, процент), то первый признак будет доминировать при поиске компонент, даже если он не самый важный. `StandardScaler` приводит все признаки к одному масштабу (обычно нулевое среднее и единичное стандартное отклонение).

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# service_pca - наш DataFrame с созданными признаками подачи (отношениями)
# Удалим строки с пропущенными значениями, если они есть
service_pca.dropna(inplace=True)

# Сохраняем индексы для последующего использования
original_indices = service_pca.index

# Данные для масштабирования (только числовые значения)
x = service_pca.values

# 1. Создаем и обучаем StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x) # Масштабируем данные

print("Данные после масштабирования (первые 5 строк, первые 3 признака):")
print(x_scaled[:5, :3])
```

**Объяснение:**
*   `StandardScaler()` создает объект для масштабирования.
*   `fit_transform(x)` одновременно вычисляет среднее и стандартное отклонение для каждого столбца в `x` и применяет преобразование, возвращая массив `x_scaled` с масштабированными данными.

**Шаг 1: Применение PCA**

Теперь, когда данные масштабированы, мы можем применить PCA. В нашем проекте мы хотим сжать все показатели подачи в *одну* главную компоненту (`n_components=1`).

```python
# 2. Создаем объект PCA
# Мы хотим получить 1 главный компонент для рейтинга подачи
pca_service = PCA(n_components=1)

# 3. Обучаем PCA на масштабированных данных и преобразуем их
# princomp_service1 - это массив с одним столбцом (наш PCA рейтинг подачи)
princomp_service1 = pca_service.fit_transform(x_scaled)

# 4. Проверяем, какую долю исходной вариативности объясняет наш компонент
explained_variance = pca_service.explained_variance_ratio_

print(f"Доля объясненной дисперсии 1-м компонентом: {explained_variance[0]:.2f}")
print("\nРезультат PCA (первые 5 значений рейтинга подачи):")
print(princomp_service1[:5])
```

**Объяснение:**
*   `PCA(n_components=1)` создает объект PCA, настроенный на поиск одной главной компоненты.
*   `fit_transform(x_scaled)` обучает PCA на масштабированных данных (находит главную компоненту) и сразу же преобразует `x_scaled`, проецируя его на эту компоненту. Результат – массив `princomp_service1`.
*   `explained_variance_ratio_` показывает, какой процент "информации" (дисперсии) из исходного набора признаков подачи содержится в нашей одной главной компоненте. В скрипте `Features_Extraction_PCA_Serve_Return_Ratings.py` это значение составляет около 45% для подачи и 50% для приема – это приемлемо, учитывая, что мы сильно сжимаем данные.

**Шаг 2: Добавление результатов PCA в основной DataFrame**

Полученные значения PCA (например, `princomp_service1`) представляют собой обобщенный рейтинг подачи (или приема) для каждого матча. Нам нужно добавить их обратно в наш основной DataFrame `matches_df`.

```python
# Создаем новые столбцы в основном DataFrame
matches_df['service_pca_1'] = None # Для PCA подачи игрока 1
matches_df['return_pca_1'] = None  # Для PCA приема игрока 1
matches_df['service_pca_2'] = None # Для PCA подачи игрока 2
matches_df['return_pca_2'] = None  # Для PCA приема игрока 2

# Создаем DataFrame из результата PCA и исходных индексов
pca_results_df = pd.DataFrame(princomp_service1, index=original_indices, columns=['pc'])

# Добавляем результаты в matches_df по соответствующим индексам
for i in range(len(pca_results_df)):
    k = pca_results_df.index[i] # Получаем исходный индекс строки
    # Записываем значение PCA в столбец 'service_pca_1' для этой строки
    matches_df.at[k, 'service_pca_1'] = pca_results_df.iloc[i]['pc']

# Аналогично делаем для PCA приема и для второго игрока...

print("\nПервые строки matches_df с добавленным PCA рейтинга подачи игрока 1:")
print(matches_df[['player_id', 'opponent_id', 'service_pca_1']].head())
```

**Объяснение:**
*   Мы создаем пустые столбцы в `matches_df` для хранения результатов PCA.
*   Мы используем `original_indices`, которые мы сохранили до удаления строк с `NaN` и масштабирования, чтобы правильно сопоставить результаты PCA (`princomp_service1`) с исходными строками в `matches_df`.
*   Цикл проходит по результатам PCA и записывает каждое значение в нужную ячейку `matches_df`.

Теперь у нас есть компактные числовые показатели (`service_pca_1`, `return_pca_1`, `service_pca_2`, `return_pca_2`), которые обобщают эффективность подачи и приема каждого игрока в каждом матче.

## Общая схема процесса

Весь процесс создания признаков и применения PCA можно представить так:

```mermaid
graph LR
    A[Сырые данные матча (final_df)] -- Статистика --> B(Выбор релевантных стат.: эйсы, подачи, очки);
    B -- Расчет соотношений --> C{Создание признаков (Feature Engineering): % попадания, % выигранных очков};
    C -- Разделение по типу --> D1[Признаки подачи (несколько столбцов)];
    C -- Разделение по типу --> D2[Признаки приема (несколько столбцов)];
    D1 -- Масштабирование --> E1(StandardScaler для подачи);
    D2 -- Масштабирование --> E2(StandardScaler для приема);
    E1 -- PCA --> F1{PCA Подачи (n_components=1)};
    E2 -- PCA --> F2{PCA Приема (n_components=1)};
    F1 -- Результат --> G1[Рейтинг подачи PCA (1 столбец)];
    F2 -- Результат --> G2[Рейтинг приема PCA (1 столбец)];
    A -- ELO --> H[Рейтинг ELO (из Главы 2)];
    G1 -- Объединение --> I[Финальный набор признаков];
    G2 -- Объединение --> I;
    H -- Объединение --> I;

    subgraph Подготовка данных для PCA
        direction LR
        B --> C;
    end

    subgraph Применение PCA
        direction TB
        D1 --> E1 --> F1 --> G1;
        D2 --> E2 --> F2 --> G2;
    end

    subgraph Сборка финальных признаков
         direction LR
         A --> H;
         G1 --> I;
         G2 --> I;
         H --> I;
    end

```

**Пояснение диаграммы:**
1.  Из сырых данных выбираем нужную статистику.
2.  Создаем новые признаки-соотношения (Feature Engineering).
3.  Группируем признаки по типу (подача, прием).
4.  Масштабируем каждую группу признаков с помощью `StandardScaler`.
5.  Применяем PCA к каждой масштабированной группе, чтобы получить один компонент (рейтинг PCA).
6.  Объединяем полученные рейтинги PCA с рейтингами ELO (из [Главы 2](02_расчет_рейтинга_elo_.html)) и, возможно, другими признаками, чтобы сформировать финальный набор данных для обучения модели.

## Заключение

В этой главе мы сделали еще один важный шаг к подготовке данных для нашей модели машинного обучения:

*   Мы научились **создавать новые признаки** (Feature Engineering) из сырой статистики матчей, чтобы получить более информативные показатели (например, проценты и соотношения).
*   Мы познакомились с **Методом Главных Компонент (PCA)** – мощным инструментом для **снижения размерности**, который позволяет сжать группы связанных признаков (например, все показатели подачи) в один-два числовых показателя, сохраняя при этом значительную часть исходной информации.
*   Мы увидели, как применить `StandardScaler` для масштабирования данных перед PCA и как добавить результаты PCA обратно в наш основной DataFrame.

В результате наш набор данных обогатился новыми признаками: `service_pca_1`, `return_pca_1`, `service_pca_2`, `return_pca_2`, которые комплексно оценивают качество подачи и приема игроков. Вместе с рейтингами ELO из предыдущей главы, эти признаки формируют прочную основу для предсказания исходов матчей.

В следующей главе мы рассмотрим, как оценить долгосрочную производительность игроков и как учесть историю их личных встреч (Head-to-Head, H2H).

**Далее:** [Глава 4: Оценка производительности и H2H](04_оценка_производительности_и_h2h_.html)

---